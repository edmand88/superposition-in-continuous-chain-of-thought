{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec89c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ccot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (embedding): Embedding(151936, 1536)\n",
       "  (unembedding): Linear(in_features=1536, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ALWAYS RUN THIS CELL FIRST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from load_embeddings import load_embedding_model\n",
    "from utils import find_similar_embeddings, prompt_to_embeddings, find_similar_logits, get_token_embedding, token_len_one_verifier, test_combinations\n",
    "\n",
    "model, tokenizer = load_embedding_model(151936, 1536, \"embeddings_qwen.pth\", \"unembeddings_qwen.pth\", \"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd7caf",
   "metadata": {},
   "source": [
    "*Colors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f975941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'black', 'white', 'gray', 'cyan', 'gold', 'silver', 'cream', 'tan', 'amber', 'azure', 'rose', 'ruby']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token colors\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\", \"black\", \"white\", \"gray\", \"cyan\", \"gold\", \"silver\", \"cream\", \"tan\", \"amber\", \"azure\", \"rose\", \"ruby\"]\n",
    "\n",
    "one_tok_colors = []\n",
    "for color in colors:\n",
    "    if token_len_one_verifier(tokenizer, color):\n",
    "        one_tok_colors.append(color)\n",
    "\n",
    "print(one_tok_colors)\n",
    "print(len(colors))\n",
    "print(len(one_tok_colors))\n",
    "\n",
    "#Getting the embeddings\n",
    "color_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8a1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "2.991000000000002\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.8000000000000047\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.246999999999963\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "4.32699999999996\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "4.047999999999968\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "3.2219999999999582\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "2.476999999999962\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "1.7599999999999782\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "0.9470000000000007\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "0.44100000000000034\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "0.18900000000000014\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "0.050000000000000024\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "0.02800000000000001\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "0.007\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_colors, color_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ec60f",
   "metadata": {},
   "source": [
    "*Animals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1124e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'lion', 'bear', 'wolf', 'fox', 'cow', 'pig', 'horse', 'deer', 'mouse', 'rat', 'rabbit', 'monkey', 'snake', 'frog', 'duck', 'hawk', 'fish', 'owl']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token animals\n",
    "animals = [\"cat\", \"dog\", \"lion\", \"bear\", \"wolf\", \"fox\", \"cow\", \"pig\", \"horse\", \"deer\", \"mouse\", \"rat\", \"rabbit\", \"monkey\", \"snake\", \"frog\", \"duck\", \"hawk\", \"fish\", \"owl\"]\n",
    "\n",
    "one_tok_animals = []\n",
    "for animal in animals:\n",
    "    if token_len_one_verifier(tokenizer, animal):\n",
    "        one_tok_animals.append(animal)\n",
    "\n",
    "print(one_tok_animals)\n",
    "print(len(animals))\n",
    "print(len(one_tok_animals))\n",
    "\n",
    "#Getting the embeddings\n",
    "animal_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_animals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4491cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.9250000000000065\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.244999999999963\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "4.034999999999985\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "2.9719999999999795\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "1.8129999999999755\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "0.9110000000000007\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "0.5840000000000004\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "0.5730000000000004\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "0.5820000000000004\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "0.6300000000000004\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "0.6810000000000005\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "0.6920000000000005\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "0.6160000000000004\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "0.4250000000000003\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "0.11052631578947372\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_animals, animal_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c80eb",
   "metadata": {},
   "source": [
    "*Common verbs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e36c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'walk', 'jump', 'sit', 'stand', 'sleep', 'eat', 'drink', 'read', 'write', 'open', 'close', 'listen', 'watch', 'play', 'move', 'stop', 'go', 'come', 'think']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token verbs\n",
    "verbs = [\"run\", \"walk\", \"jump\", \"sit\", \"stand\", \"sleep\", \"eat\", \"drink\", \"read\", \"write\",\n",
    " \"open\", \"close\", \"listen\", \"watch\", \"play\", \"move\", \"stop\", \"go\", \"come\",\n",
    " \"think\"]\n",
    "\n",
    "one_tok_verbs = []\n",
    "for verb in verbs:\n",
    "    if token_len_one_verifier(tokenizer, verb):\n",
    "        one_tok_verbs.append(verb)\n",
    "\n",
    "print(one_tok_verbs)\n",
    "print(len(verbs))\n",
    "print(len(one_tok_verbs))\n",
    "\n",
    "#Getting the embeddings\n",
    "verb_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_verbs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f9d1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.8820000000000054\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "3.993999999999976\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "3.4039999999999653\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "2.219999999999968\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "1.2109999999999899\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "0.5850000000000004\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "0.23500000000000018\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "0.06600000000000004\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "0.011000000000000003\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_verbs, verb_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4495cc",
   "metadata": {},
   "source": [
    "*Geometric objects*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a5a083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circle', 'square', 'triangle', 'rectangle', 'oval', 'cube', 'cone', 'sphere', 'ellipse', 'line', 'curve', 'ring', 'dot', 'cross', 'loop', 'polygon', 'point', 'plane', 'edge', 'angle']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token geometry\n",
    "geo_objects = [\"circle\", \"square\", \"triangle\", \"rectangle\", \"oval\", \"cube\", \"cone\", \"sphere\", \"ellipse\", \"line\", \"curve\", \"ring\", \"dot\", \"cross\", \"loop\", \"polygon\", \"point\", \"plane\", \"edge\", \"angle\"]\n",
    "\n",
    "one_tok_geo_objects = []\n",
    "for geo_object in geo_objects:\n",
    "    if token_len_one_verifier(tokenizer, geo_object):\n",
    "        one_tok_geo_objects.append(geo_object)\n",
    "\n",
    "print(one_tok_geo_objects)\n",
    "print(len(geo_objects))\n",
    "print(len(one_tok_geo_objects))\n",
    "\n",
    "#Getting the embeddings\n",
    "geo_object_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_geo_objects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6dabcebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.898999999999914\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.419999999999945\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "5.38599999999994\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "4.724999999999947\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "3.9159999999999595\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "2.998999999999962\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "2.19199999999997\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "1.4169999999999852\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "0.9220000000000007\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "0.7570000000000006\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "0.6140000000000004\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "0.4850000000000004\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "0.34300000000000025\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "0.1526315789473684\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_geo_objects, geo_object_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3387c9",
   "metadata": {},
   "source": [
    "*Body parts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3029fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head', 'hand', 'arm', 'leg', 'foot', 'eye', 'ear', 'mouth', 'back', 'neck', 'finger', 'chest', 'hip', 'hair', 'skin', 'face', 'throat', 'brain', 'heart', 'lung']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token body parts\n",
    "body_parts = [\"head\", \"hand\", \"arm\", \"leg\", \"foot\", \"eye\", \"ear\", \"mouth\", \"back\", \"neck\", \"finger\", \"chest\", \"hip\", \"hair\", \"skin\", \"face\", \"throat\", \"brain\", \"heart\", \"lung\"]\n",
    "one_tok_body_parts = []\n",
    "for body_part in body_parts:\n",
    "    if token_len_one_verifier(tokenizer, body_part):\n",
    "        one_tok_body_parts.append(body_part)\n",
    "\n",
    "print(one_tok_body_parts)\n",
    "print(len(body_parts))\n",
    "print(len(one_tok_body_parts))\n",
    "\n",
    "#Getting the embeddings\n",
    "body_part_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_body_parts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c53a1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.9680000000000035\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.48799999999994\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "4.301999999999961\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "3.8409999999999895\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "3.31799999999999\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "2.8869999999999756\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "2.684999999999971\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "2.428999999999973\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "2.08899999999997\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "1.711999999999978\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "1.3189999999999884\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "0.9620000000000007\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "0.5170000000000003\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "0.17900000000000013\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "0.010526315789473684\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_body_parts, body_part_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daaa10d",
   "metadata": {},
   "source": [
    "*Adjectives*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "77f9ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'bad', 'new', 'old', 'big', 'small', 'long', 'short', 'high', 'low', 'wide', 'deep', 'hot', 'cold', 'dark', 'light', 'soft', 'hard', 'fast', 'slow']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token adjectives\n",
    "adjectives = [\"good\", \"bad\", \"new\", \"old\", \"big\", \"small\", \"long\", \"short\", \"high\", \"low\", \"wide\", \"deep\", \"hot\", \"cold\", \"dark\", \"light\", \"soft\", \"hard\", \"fast\", \"slow\"]\n",
    "one_tok_adjectives = []\n",
    "for adjective in adjectives:\n",
    "    if token_len_one_verifier(tokenizer, adjective):\n",
    "        one_tok_adjectives.append(adjective)\n",
    "\n",
    "print(one_tok_adjectives)\n",
    "print(len(adjectives))\n",
    "print(len(one_tok_adjectives))\n",
    "\n",
    "#Getting the embeddings\n",
    "adjective_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_adjectives}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3c76fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.9940000000000033\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.86299999999992\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.40099999999995\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "5.54299999999994\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "5.325999999999946\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "4.970999999999951\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "4.482999999999961\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "4.045999999999967\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "3.3189999999999666\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "2.518999999999971\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "1.8289999999999742\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "1.106999999999996\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "0.3910000000000003\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "0.08800000000000006\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "0.0\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_adjectives, adjective_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3ceb6",
   "metadata": {},
   "source": [
    "*Pronouns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef84c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'mine', 'his', 'its', 'ours', 'this', 'that', 'these', 'those']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token pronouns\n",
    "pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"mine\", \"his\", \"its\", \"ours\", \"this\", \"that\", \"these\", \"those\"]\n",
    "\n",
    "one_tok_pronouns = []\n",
    "for pronoun in pronouns:\n",
    "    if token_len_one_verifier(tokenizer, pronoun):\n",
    "        one_tok_pronouns.append(pronoun)\n",
    "\n",
    "print(one_tok_pronouns)\n",
    "print(len(pronouns))\n",
    "print(len(one_tok_pronouns))\n",
    "\n",
    "#Getting the embeddings\n",
    "pronoun_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_pronouns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a18c1f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "2.985\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.8870000000000062\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.677999999999921\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.400999999999943\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "5.988999999999934\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "6.476999999999944\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "6.900999999999983\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "7.187999999999989\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "7.393999999999954\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "7.519999999999973\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "7.55299999999994\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "7.530999999999964\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "7.486999999999968\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "7.281999999999959\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "7.334999999999969\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "7.278947368421061\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "7.299999999999999\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "7.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_pronouns, pronoun_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9283a",
   "metadata": {},
   "source": [
    "*Prepositions & Conjunctions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b31b0639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'in', 'on', 'at', 'to', 'for', 'from', 'with', 'by', 'up', 'down', 'out', 'over', 'and', 'or', 'but', 'so', 'if', 'than', 'as']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token prepositions & conjunctions\n",
    "p_cs = [\"of\", \"in\", \"on\", \"at\", \"to\", \"for\", \"from\", \"with\", \"by\", \"up\", \"down\", \"out\", \"over\", \"and\", \"or\", \"but\", \"so\", \"if\", \"than\", \"as\"]\n",
    "\n",
    "one_tok_p_cs = []\n",
    "for p_c in p_cs:\n",
    "    if token_len_one_verifier(tokenizer, p_c):\n",
    "        one_tok_p_cs.append(p_c)\n",
    "\n",
    "print(one_tok_p_cs)\n",
    "print(len(p_cs))\n",
    "print(len(one_tok_p_cs))\n",
    "\n",
    "#Getting the embeddings\n",
    "p_c_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_p_cs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b792ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "6.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.999999999999879\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "8.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "9.000000000000156\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.999999999999831\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.999999999999847\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "12.00000000000001\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.99599999999991\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.99099999999976\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "14.984000000000181\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "15.955000000000025\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "16.878999999999795\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "17.699999999999985\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "18.25\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "19.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_p_cs, p_c_embeddings, combination_sizes, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
