{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ec89c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ccot/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (embedding): Embedding(151936, 2560)\n",
       "  (unembedding): Linear(in_features=2560, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ALWAYS RUN THIS CELL FIRST\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from load_embeddings import load_embedding_model\n",
    "from utils import find_similar_embeddings, prompt_to_embeddings, find_similar_logits, get_token_embedding, token_len_one_verifier, test_combinations\n",
    "\n",
    "model, tokenizer = load_embedding_model(151936, 2560, \"Qwen_Qwen3-4B-Instruct-2507_embeddings_qwen.pth\", \"Qwen_Qwen3-4B-Instruct-2507_unembeddings_qwen.pth\", \"Qwen/Qwen3-4B-Instruct-2507\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94dd7caf",
   "metadata": {},
   "source": [
    "*Colors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f975941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['red', 'blue', 'green', 'yellow', 'orange', 'purple', 'pink', 'brown', 'black', 'white', 'gray', 'cyan', 'gold', 'silver', 'cream', 'tan', 'amber', 'azure', 'rose', 'ruby']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token colors\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\", \"black\", \"white\", \"gray\", \"cyan\", \"gold\", \"silver\", \"cream\", \"tan\", \"amber\", \"azure\", \"rose\", \"ruby\"]\n",
    "\n",
    "one_tok_colors = []\n",
    "for color in colors:\n",
    "    if token_len_one_verifier(tokenizer, color):\n",
    "        one_tok_colors.append(color)\n",
    "\n",
    "print(one_tok_colors)\n",
    "print(len(colors))\n",
    "print(len(one_tok_colors))\n",
    "\n",
    "#Getting the embeddings\n",
    "color_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_colors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb8a1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.991000000000002\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.920999999999895\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.734999999999973\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.432000000000043\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "8.967000000000038\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "9.58999999999998\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "10.101999999999906\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "10.567999999999897\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "10.973999999999876\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "11.332999999999913\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "11.698999999999915\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "11.957999999999933\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "12.173684210526321\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "12.5\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "13.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_colors, color_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613ec60f",
   "metadata": {},
   "source": [
    "*Animals*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1124e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'dog', 'lion', 'bear', 'wolf', 'fox', 'cow', 'pig', 'horse', 'deer', 'mouse', 'rat', 'rabbit', 'monkey', 'snake', 'frog', 'duck', 'hawk', 'fish', 'owl']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token animals\n",
    "animals = [\"cat\", \"dog\", \"lion\", \"bear\", \"wolf\", \"fox\", \"cow\", \"pig\", \"horse\", \"deer\", \"mouse\", \"rat\", \"rabbit\", \"monkey\", \"snake\", \"frog\", \"duck\", \"hawk\", \"fish\", \"owl\"]\n",
    "\n",
    "one_tok_animals = []\n",
    "for animal in animals:\n",
    "    if token_len_one_verifier(tokenizer, animal):\n",
    "        one_tok_animals.append(animal)\n",
    "\n",
    "print(one_tok_animals)\n",
    "print(len(animals))\n",
    "print(len(one_tok_animals))\n",
    "\n",
    "#Getting the embeddings\n",
    "animal_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_animals}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4491cfab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "6.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.999999999999879\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "8.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.999000000000155\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.994999999999834\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.946999999999848\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "11.86199999999998\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.712999999999948\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.469999999999848\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "14.245999999999864\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "15.070000000000114\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "15.808000000000062\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "16.40000000000002\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "17.099999999999998\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "18.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_animals, animal_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c80eb",
   "metadata": {},
   "source": [
    "*Common verbs*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38e36c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'walk', 'jump', 'sit', 'stand', 'sleep', 'eat', 'drink', 'read', 'write', 'open', 'close', 'listen', 'watch', 'play', 'move', 'stop', 'go', 'come', 'think']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token verbs\n",
    "verbs = [\"run\", \"walk\", \"jump\", \"sit\", \"stand\", \"sleep\", \"eat\", \"drink\", \"read\", \"write\",\n",
    " \"open\", \"close\", \"listen\", \"watch\", \"play\", \"move\", \"stop\", \"go\", \"come\",\n",
    " \"think\"]\n",
    "\n",
    "one_tok_verbs = []\n",
    "for verb in verbs:\n",
    "    if token_len_one_verifier(tokenizer, verb):\n",
    "        one_tok_verbs.append(verb)\n",
    "\n",
    "print(one_tok_verbs)\n",
    "print(len(verbs))\n",
    "print(len(one_tok_verbs))\n",
    "\n",
    "#Getting the embeddings\n",
    "verb_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_verbs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f9d1c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "6.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.999999999999879\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.999000000000007\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.993000000000155\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.985999999999837\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.974999999999849\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "11.954000000000008\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.906999999999924\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.860999999999787\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "14.8220000000001\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "15.74600000000006\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "16.643999999999853\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "17.49473684210523\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "18.25\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "19.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_verbs, verb_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4495cc",
   "metadata": {},
   "source": [
    "*Geometric objects*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a5a083b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circle', 'square', 'triangle', 'rectangle', 'oval', 'cube', 'cone', 'sphere', 'ellipse', 'line', 'curve', 'ring', 'dot', 'cross', 'loop', 'polygon', 'point', 'plane', 'edge', 'angle']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token geometry\n",
    "geo_objects = [\"circle\", \"square\", \"triangle\", \"rectangle\", \"oval\", \"cube\", \"cone\", \"sphere\", \"ellipse\", \"line\", \"curve\", \"ring\", \"dot\", \"cross\", \"loop\", \"polygon\", \"point\", \"plane\", \"edge\", \"angle\"]\n",
    "\n",
    "one_tok_geo_objects = []\n",
    "for geo_object in geo_objects:\n",
    "    if token_len_one_verifier(tokenizer, geo_object):\n",
    "        one_tok_geo_objects.append(geo_object)\n",
    "\n",
    "print(one_tok_geo_objects)\n",
    "print(len(geo_objects))\n",
    "print(len(one_tok_geo_objects))\n",
    "\n",
    "#Getting the embeddings\n",
    "geo_object_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_geo_objects}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6dabcebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.998000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.984999999999883\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.939999999999997\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.831000000000108\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.626999999999926\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.30699999999992\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "10.88299999999989\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "11.218999999999896\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "11.609999999999877\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "11.856999999999902\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "12.162999999999935\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "12.391999999999959\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "12.705263157894766\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "13.100000000000001\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "13.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_geo_objects, geo_object_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3387c9",
   "metadata": {},
   "source": [
    "*Body parts*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3029fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['head', 'hand', 'arm', 'leg', 'foot', 'eye', 'ear', 'mouth', 'back', 'neck', 'finger', 'chest', 'hip', 'hair', 'skin', 'face', 'throat', 'brain', 'heart', 'lung']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token body parts\n",
    "body_parts = [\"head\", \"hand\", \"arm\", \"leg\", \"foot\", \"eye\", \"ear\", \"mouth\", \"back\", \"neck\", \"finger\", \"chest\", \"hip\", \"hair\", \"skin\", \"face\", \"throat\", \"brain\", \"heart\", \"lung\"]\n",
    "one_tok_body_parts = []\n",
    "for body_part in body_parts:\n",
    "    if token_len_one_verifier(tokenizer, body_part):\n",
    "        one_tok_body_parts.append(body_part)\n",
    "\n",
    "print(one_tok_body_parts)\n",
    "print(len(body_parts))\n",
    "print(len(one_tok_body_parts))\n",
    "\n",
    "#Getting the embeddings\n",
    "body_part_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_body_parts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c53a1f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "6.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.999999999999879\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "8.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.994000000000154\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.984999999999834\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.940999999999852\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "11.826999999999982\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.581999999999933\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.271999999999869\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "13.787999999999903\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "14.225999999999917\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "14.64999999999995\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "15.289473684210542\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "16.100000000000005\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "17.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_body_parts, body_part_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daaa10d",
   "metadata": {},
   "source": [
    "*Adjectives*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77f9ca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'bad', 'new', 'old', 'big', 'small', 'long', 'short', 'high', 'low', 'wide', 'deep', 'hot', 'cold', 'dark', 'light', 'soft', 'hard', 'fast', 'slow']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token adjectives\n",
    "adjectives = [\"good\", \"bad\", \"new\", \"old\", \"big\", \"small\", \"long\", \"short\", \"high\", \"low\", \"wide\", \"deep\", \"hot\", \"cold\", \"dark\", \"light\", \"soft\", \"hard\", \"fast\", \"slow\"]\n",
    "one_tok_adjectives = []\n",
    "for adjective in adjectives:\n",
    "    if token_len_one_verifier(tokenizer, adjective):\n",
    "        one_tok_adjectives.append(adjective)\n",
    "\n",
    "print(one_tok_adjectives)\n",
    "print(len(adjectives))\n",
    "print(len(one_tok_adjectives))\n",
    "\n",
    "#Getting the embeddings\n",
    "adjective_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_adjectives}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c76fa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.999000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.975999999999882\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.956999999999999\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.92400000000013\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.893999999999872\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.795999999999847\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "11.729999999999965\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.676999999999952\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.543999999999837\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "14.429999999999946\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "15.302000000000145\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "16.20299999999995\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "17.11052631578946\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "18.05\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "19.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_adjectives, adjective_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3ceb6",
   "metadata": {},
   "source": [
    "*Pronouns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ef84c856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'mine', 'his', 'its', 'ours', 'this', 'that', 'these', 'those']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token pronouns\n",
    "pronouns = [\"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \"her\", \"us\", \"them\", \"mine\", \"his\", \"its\", \"ours\", \"this\", \"that\", \"these\", \"those\"]\n",
    "\n",
    "one_tok_pronouns = []\n",
    "for pronoun in pronouns:\n",
    "    if token_len_one_verifier(tokenizer, pronoun):\n",
    "        one_tok_pronouns.append(pronoun)\n",
    "\n",
    "print(one_tok_pronouns)\n",
    "print(len(pronouns))\n",
    "print(len(one_tok_pronouns))\n",
    "\n",
    "#Getting the embeddings\n",
    "pronoun_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_pronouns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a18c1f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "3.9970000000000034\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.969999999999915\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.9359999999999955\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.869999999999902\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.784999999999974\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.730000000000077\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.584999999999972\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.502999999999865\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "11.379999999999875\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.254999999999974\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.205999999999884\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "14.116999999999832\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "15.027000000000132\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "15.962000000000018\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "16.91578947368421\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "17.75\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "19.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_pronouns, pronoun_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd9283a",
   "metadata": {},
   "source": [
    "*Prepositions & Conjunctions*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b31b0639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['of', 'in', 'on', 'at', 'to', 'for', 'from', 'with', 'by', 'up', 'down', 'out', 'over', 'and', 'or', 'but', 'so', 'if', 'than', 'as']\n",
      "20\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#Getting one token prepositions & conjunctions\n",
    "p_cs = [\"of\", \"in\", \"on\", \"at\", \"to\", \"for\", \"from\", \"with\", \"by\", \"up\", \"down\", \"out\", \"over\", \"and\", \"or\", \"but\", \"so\", \"if\", \"than\", \"as\"]\n",
    "\n",
    "one_tok_p_cs = []\n",
    "for p_c in p_cs:\n",
    "    if token_len_one_verifier(tokenizer, p_c):\n",
    "        one_tok_p_cs.append(p_c)\n",
    "\n",
    "print(one_tok_p_cs)\n",
    "print(len(p_cs))\n",
    "print(len(one_tok_p_cs))\n",
    "\n",
    "#Getting the embeddings\n",
    "p_c_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_p_cs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b792ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "1.0000000000000002\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.000000000000004\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "6.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.999999999999879\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "8.000000000000005\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "9.000000000000156\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.999999999999831\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.999999999999847\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "12.00000000000001\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "12.99999999999991\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "13.999999999999758\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "15.00000000000019\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "16.00000000000001\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "16.99999999999978\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "17.99999999999998\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "18.999999999999993\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "20.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_p_cs, p_c_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f951f0",
   "metadata": {},
   "source": [
    "*Random words*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6669e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cube', 'at', 'horse', 'he', 'green', 'dot', 'cream', 'them', 'cold', 'play', 'back', 'azure', 'come', 'good', 'hand', 'snake', 'yellow', 'fox', 'we', 'plane', 'gray', 'read', 'hot', 'amber', 'eat', 'walk', 'mine', 'than', 'ear', 'to']\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "#Getting one token prepositions & conjunctions\n",
    "alls = [\"red\", \"blue\", \"green\", \"yellow\", \"orange\", \"purple\", \"pink\", \"brown\", \"black\", \"white\", \"gray\", \"cyan\", \"gold\", \n",
    "        \"silver\", \"cream\", \"tan\", \"amber\", \"azure\", \"rose\", \"ruby\", \"cat\", \"dog\", \"lion\", \"bear\", \"wolf\", \"fox\", \"cow\", \n",
    "        \"pig\", \"horse\", \"deer\", \"mouse\", \"rat\", \"rabbit\", \"monkey\", \"snake\", \"frog\", \"duck\", \"hawk\", \"fish\", \"owl\", \"run\", \n",
    "        \"walk\", \"jump\", \"sit\", \"stand\", \"sleep\", \"eat\", \"drink\", \"read\", \"write\", \"open\", \"close\", \"listen\", \"watch\", \"play\", \n",
    "        \"move\", \"stop\", \"go\", \"come\", \"think\", \"circle\", \"square\", \"triangle\", \"rectangle\", \"oval\", \"cube\", \"cone\", \"sphere\", \n",
    "        \"ellipse\", \"line\", \"curve\", \"ring\", \"dot\", \"cross\", \"loop\", \"polygon\", \"point\", \"plane\", \"edge\", \"angle\", \"head\", \"hand\", \n",
    "        \"arm\", \"leg\", \"foot\", \"eye\", \"ear\", \"mouth\", \"back\", \"neck\", \"finger\", \"chest\", \"hip\", \"hair\", \"skin\", \"face\", \"throat\", \n",
    "        \"brain\", \"heart\", \"lung\", \"good\", \"bad\", \"new\", \"old\", \"big\", \"small\", \"long\", \"short\", \"high\", \"low\", \"wide\", \"deep\", \n",
    "        \"hot\", \"cold\", \"dark\", \"light\", \"soft\", \"hard\", \"fast\", \"slow\", \"I\", \"you\", \"he\", \"she\", \"it\", \"we\", \"they\", \"me\", \"him\", \n",
    "        \"her\", \"us\", \"them\", \"mine\", \"his\", \"its\", \"ours\", \"this\", \"that\", \"these\", \"those\", \"of\", \"in\", \"on\", \"at\", \"to\", \"for\", \n",
    "        \"from\", \"with\", \"by\", \"up\", \"down\", \"out\", \"over\", \"and\", \"or\", \"but\", \"so\", \"if\", \"than\", \"as\"]\n",
    "\n",
    "alls = sorted(alls)\n",
    "alls = random.sample(alls, 30)\n",
    "\n",
    "one_tok_alls = []\n",
    "for all in alls:\n",
    "    if token_len_one_verifier(tokenizer, all):\n",
    "        one_tok_alls.append(all)\n",
    "\n",
    "print(one_tok_alls)\n",
    "print(len(alls))\n",
    "print(len(one_tok_alls))\n",
    "\n",
    "#Getting the embeddings\n",
    "all_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_alls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bde3735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "0.9999999999999999\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.0000000000000133\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.955000000000001\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.863999999999905\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.7079999999999655\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.537000000000054\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.357999999999993\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.124999999999918\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "10.843999999999854\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "11.548999999999893\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "12.214999999999941\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "12.867999999999908\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "13.449999999999886\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "14.008999999999904\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "14.51299999999998\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "15.033000000000001\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "15.507000000000026\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_alls, all_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddbbc0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hot', 'cream', 'we', 'ear', 'amber', 'read', 'plane', 'azure', 'hand', 'fox', 'good', 'to', 'play', 'mine', 'come', 'horse', 'them', 'at', 'cold', 'back', 'walk', 'green', 'yellow', 'cube', 'snake', 'dot', 'eat', 'he', 'gray', 'than']\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "alls = random.sample(alls, 30)\n",
    "\n",
    "one_tok_alls = []\n",
    "for all in alls:\n",
    "    if token_len_one_verifier(tokenizer, all):\n",
    "        one_tok_alls.append(all)\n",
    "\n",
    "print(one_tok_alls)\n",
    "print(len(alls))\n",
    "print(len(one_tok_alls))\n",
    "\n",
    "#Getting the embeddings\n",
    "all_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_alls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f99022b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "0.9999999999999999\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.0000000000000133\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.999999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.969\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.864999999999901\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.732999999999974\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.548000000000062\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.33899999999999\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.136999999999901\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "10.83299999999987\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "11.533999999999889\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "12.277999999999938\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "12.851999999999919\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "13.427999999999884\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "13.955999999999905\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "14.523999999999964\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "15.082000000000011\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "15.531000000000015\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_alls, all_embeddings, combination_sizes, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c705c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azure', 'good', 'read', 'plane', 'green', 'amber', 'mine', 'cube', 'cream', 'walk', 'hot', 'dot', 'hand', 'come', 'back', 'yellow', 'fox', 'to', 'cold', 'play', 'horse', 'ear', 'snake', 'than', 'he', 'we', 'at', 'them', 'eat', 'gray']\n",
      "30\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "alls = random.sample(alls, 30)\n",
    "\n",
    "one_tok_alls = []\n",
    "for all in alls:\n",
    "    if token_len_one_verifier(tokenizer, all):\n",
    "        one_tok_alls.append(all)\n",
    "\n",
    "print(one_tok_alls)\n",
    "print(len(alls))\n",
    "print(len(one_tok_alls))\n",
    "\n",
    "#Getting the embeddings\n",
    "all_embeddings = {c: get_token_embedding(model, tokenizer, c) for c in one_tok_alls}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c618a3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combination size: 1\n",
      "0.9999999999999999\n",
      "\n",
      "\n",
      "Combination size: 2\n",
      "2.0000000000000133\n",
      "\n",
      "\n",
      "Combination size: 3\n",
      "3.0000000000000027\n",
      "\n",
      "\n",
      "Combination size: 4\n",
      "4.000000000000003\n",
      "\n",
      "\n",
      "Combination size: 5\n",
      "4.998999999999916\n",
      "\n",
      "\n",
      "Combination size: 6\n",
      "5.96\n",
      "\n",
      "\n",
      "Combination size: 7\n",
      "6.872999999999911\n",
      "\n",
      "\n",
      "Combination size: 8\n",
      "7.699999999999967\n",
      "\n",
      "\n",
      "Combination size: 9\n",
      "8.54500000000006\n",
      "\n",
      "\n",
      "Combination size: 10\n",
      "9.312000000000008\n",
      "\n",
      "\n",
      "Combination size: 11\n",
      "10.121999999999916\n",
      "\n",
      "\n",
      "Combination size: 12\n",
      "10.828999999999857\n",
      "\n",
      "\n",
      "Combination size: 13\n",
      "11.578999999999889\n",
      "\n",
      "\n",
      "Combination size: 14\n",
      "12.22499999999995\n",
      "\n",
      "\n",
      "Combination size: 15\n",
      "12.890999999999917\n",
      "\n",
      "\n",
      "Combination size: 16\n",
      "13.459999999999885\n",
      "\n",
      "\n",
      "Combination size: 17\n",
      "14.004999999999912\n",
      "\n",
      "\n",
      "Combination size: 18\n",
      "14.57899999999998\n",
      "\n",
      "\n",
      "Combination size: 19\n",
      "15.054000000000038\n",
      "\n",
      "\n",
      "Combination size: 20\n",
      "15.556000000000045\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Testing combinations of different sizes from 1 to 20\n",
    "combination_sizes = list(range(1, 21))\n",
    "results = test_combinations(model, tokenizer, one_tok_alls, all_embeddings, combination_sizes, 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
